# CI/CD/CT en el Proyecto de ClasificaciÃ³n de Tweets

## ğŸ“„ IntroducciÃ³n
El objetivo del pipeline CI/CD/CT implementado es garantizar la calidad del cÃ³digo, la automatizaciÃ³n del reentrenamiento y empaquetado del modelo, y su preparaciÃ³n para despliegue. Esto se logra mediante la integraciÃ³n de GitHub Actions como sistema de orquestaciÃ³n, lo cual permite mantener buenas prÃ¡cticas de ingenierÃ­a de software y Machine Learning dentro del flujo de desarrollo y operaciones.

---

## ğŸ“† Workflow `ci_validation.yml` (IntegraciÃ³n Continua)

### âœ… PropÃ³sito
Este workflow verifica automÃ¡ticamente la calidad y consistencia del cÃ³digo en cada modificaciÃ³n relevante sobre la rama `main`.

### ğŸ” Disparadores
- `push` a `main`
- `pull_request` hacia `main`

### âš–ï¸ Pasos Clave
1. **`checkout`**: Clona el repositorio.
2. **`setup-python`**: Configura Python 3.12.
3. **InstalaciÃ³n de Dependencias**: Usando `requirements.txt`.
4. **`flake8` (Linting)**: Revisa la calidad de cÃ³digo en la carpeta `src/`.

### âœ¨ Extensiones Propuestas
- Incorporar pruebas unitarias utilizando `pytest`.
- ValidaciÃ³n de notebooks con `nbval` o `papermill`.
- ConstrucciÃ³n de imagen Docker en entorno de prueba.
- EjecuciÃ³n de tests bÃ¡sicos sobre la imagen construida.

---

## ğŸ› ï¸ Workflow `manual_retrain_pipeline.yml` (Entrenamiento y "Despliegue" Continuo)

### âœ… PropÃ³sito
Automatizar el reentrenamiento del modelo desde cero, ejecutando el pipeline completo de procesamiento, generaciÃ³n de features, pseudo-etiquetado y clasificaciÃ³n, junto con la creaciÃ³n de una nueva imagen Docker conteniendo los artefactos generados.

### ğŸ” Disparador
- Manual mediante `workflow_dispatch`, con un parÃ¡metro `log_level` como input.

### âš–ï¸ Pasos Clave
1. **`checkout` del repositorio**.
2. **ConfiguraciÃ³n del entorno**: InstalaciÃ³n de dependencias.
3. **EjecuciÃ³n de scripts del pipeline**, en orden:
   - `data_ingestion.py`
   - `data_preparation.py`
   - `preprocessing.py`
   - `feature_engineering.py`
   - `clustering.py`
   - `model_training.py`
4. **ConstrucciÃ³n de imagen Docker** con los artefactos generados (modelos, datos, reportes).
5. **Etiquetado de la imagen** con `github.run_id`.
6. **Subida de artefactos** al job de GitHub Actions (accesible desde la pestaÃ±a "Actions").

### ğŸŒ¿ Estrategia de Reentrenamiento
Gracias a que `data_ingestion.py` puede re-descargar o manejar nuevos datos, y todo el pipeline se ejecuta desde cero, este workflow permite realizar un reentrenamiento completo con:
- Cambios en el cÃ³digo
- Nuevos datos
- Nuevas configuraciones de clustering o clasificaciÃ³n

Esto facilita la evoluciÃ³n del modelo sin necesidad de intervenciones manuales mÃ¡s allÃ¡ de ejecutar el workflow.

---

## ğŸ  Detalle del Despliegue (IntegraciÃ³n con Arquitectura de Nube)

### ğŸŒ Rol de la Imagen Docker Construida
La imagen Docker generada por `manual_retrain_pipeline.yml` contiene:
- CÃ³digo fuente actualizado
- Modelos entrenados
- Artefactos intermedios y finales del pipeline

Esta imagen se puede subir a un registro como **Amazon ECR**, y desde allÃ­ integrarse en la arquitectura propuesta en AWS (ver `ARQUITECTURA_NUBE.md` y `mlops_architecture.py`).

### âš™ï¸ Escenarios de Despliegue Posibles
- **AWS Batch / Fargate**: Ejecutar inferencia batch sobre nuevos datos usando esta imagen.
- **SageMaker Batch Transform**: Cargar la imagen como contenedor personalizado y ejecutar inferencias en lotes.
- **ActualizaciÃ³n de modelo**: Si el modelo se registra en `SageMaker Model Registry`, puede versionarse y seleccionarse para su uso en etapas de inferencia o evaluaciÃ³n futuras.

Este enfoque asegura que la versiÃ³n de modelo entrenada pueda desplegarse de forma coherente en diferentes entornos sin necesidad de reentrenamiento adicional.

---

## ğŸš€ ConclusiÃ³n
El diseÃ±o del pipeline CI/CD/CT del proyecto permite mantener la calidad del cÃ³digo, facilitar el reentrenamiento bajo demanda y habilitar un despliegue controlado de versiones del modelo, maximizando la reproducibilidad y trazabilidad. La integraciÃ³n con servicios de nube como Amazon ECR, SageMaker y AWS Batch asegura su aplicabilidad en contextos reales de producciÃ³n.
